{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and classifier (Kamran Ashraf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "import string, os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of files found ::  955\n"
     ]
    }
   ],
   "source": [
    "# Path for file reading and aspects files.\n",
    "dataDirPath = os.path.join(os.getcwd() ,'data')\n",
    "dataFilesPath = os.path.join(dataDirPath , 'dataDir')\n",
    "\n",
    "aspectsDirPath = os.path.join(os.getcwd() , 'aspectDir')\n",
    "preprocessFilesPath = os.path.join(aspectsDirPath , 'preprocessDataDir')\n",
    "unprocessFilesPath = os.path.join(aspectsDirPath , 'unprocessDataDir')\n",
    "\n",
    "# creating aspectDir and its subfolders preprocessFilesPath, unprocessFilesPath\n",
    "try:\n",
    "    os.mkdir('aspectDir')\n",
    "    os.makedirs(preprocessFilesPath)\n",
    "    os.makedirs(unprocessFilesPath)\n",
    "except:\n",
    "    print('Either dir already exits or somthing goes wrong')\n",
    "    \n",
    "# No of comments file\n",
    "NO_OF_FILES = len(os.listdir(dataFilesPath))\n",
    "print('No of files found :: ' ,NO_OF_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Defined Aspects and their frequently occurring keywords\n",
    "\n",
    "aspects = ['Value', 'Location', 'Service', 'Meal', 'Facility', 'Room', 'Quality', 'Staff', 'Surrounding', 'Others']\n",
    "\n",
    "aspectsKeywords = [['Value', 'Price', 'Amount', 'Rate', 'Cheap', 'Worth', 'Low', 'Money', 'Economical', 'Reasonable', 'Fee', 'Expensive'],\n",
    " ['Location', 'Railway', 'View', 'Station', 'Airport', 'Distance', 'Far', 'Close', 'Convenient', 'Train', 'Metro'],\n",
    " ['Service', 'Desk', 'Check-in', 'Checkout', 'Reliable', 'Fast', 'Convenient', 'Pick-up'],\n",
    " ['Meal', 'Drink', 'Breakfast', 'Spicy', 'Food', 'Tasty', 'Tea', 'Buffet', 'Bar', 'Restaurant', 'Dinner', 'Lunch', 'Brunch', 'Delicious'],\n",
    " ['Facility', 'Pool', 'Spa', 'Wifi', 'Gymnasium', 'Gym', 'Internet', 'Ample', 'Parking', 'Wireless', 'Broken'],\n",
    " ['Room', 'Bed', 'Dirty', 'Clean', 'Toilet', 'Bathroom', 'Shower', 'Dryer', 'Fridge', 'View'],\n",
    " ['Quality', 'Satisfactory', 'Ample', 'Hygienic', 'Proper', 'Ambience', 'Odour', 'Smell'],\n",
    " ['Staff', 'Good', 'Polite', 'Helpful', 'Friendly', 'Reliable', 'Quick', 'Reception', 'Manager'],\n",
    " ['Surrounding', 'Landmark', 'Monument', 'Temple', 'Mosque', 'Church', 'Restaurant', 'Diner', 'Mall', 'Market'],\n",
    " ['Paharganj', 'Delhi', 'Stay', 'Visit', 'Back', 'Driver', 'Booked', 'Agent', 'Trip']]\n",
    "\n",
    "# creating empty aspects files\n",
    "for aspect in aspects:\n",
    "    try:\n",
    "        with open(os.path.join(preprocessFilesPath, f'{aspect}.txt'), 'w', encoding=\"utf-8\") as file:\n",
    "            file.write('')\n",
    "        with open(os.path.join(unprocessFilesPath, f'{aspect}.txt'), 'w', encoding=\"utf-8\") as file:\n",
    "            file.write('')\n",
    "    except:\n",
    "        print(\"Could not write file. Try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Classifier :: uses wup_similarity \n",
    "    Wu-Palmer Similarity, which is a scoring method based on how similar the word senses are and where\n",
    "    the Synsets occur relative to each other in the hypernym tree.\n",
    "'''\n",
    "    \n",
    "def classifier(processedTokenizedSentance):\n",
    "    aspectsScore = []\n",
    "    for i in range(0,10):\n",
    "        aspectsScore.append(0)\n",
    "#   Calculate scores of aspects for the input sentance\n",
    "    for word in processedTokenizedSentance:\n",
    "        for i in range(0,10):\n",
    "            score = calWordScore(word, i)\n",
    "            aspectsScore[i] = aspectsScore[i] + score\n",
    "#   Pick the aspects with max score and returns its id\n",
    "    maxScoreIndx = 0\n",
    "    for i in range(1, 10):\n",
    "        if aspectsScore[maxScoreIndx] < aspectsScore[i]:\n",
    "            maxScoreIndx = i \n",
    "    return maxScoreIndx\n",
    "\n",
    "'''\n",
    "    Calculate score after comparing word with each word in aspectsKeywords[i] ( 'i' passed as parameter )\n",
    "    if similarity is 'none' it returns 0 as score.\n",
    "'''\n",
    "def calWordScore(word, i):\n",
    "    score = 0\n",
    "    for listWord in aspectsKeywords[i]:\n",
    "        try:\n",
    "            a = wordnet.synsets(listWord)[0]\n",
    "            b = wordnet.synsets(word)[0]\n",
    "        except:\n",
    "            # If error occurs (word spelling is incorrect or return list is empty)\n",
    "            continue\n",
    "        s = b.wup_similarity(a)\n",
    "        p = a.wup_similarity(b)\n",
    "        similarityScore = (s or p)\n",
    "    #   in case s = none or p = none , it takes the one with score\n",
    "        if similarityScore:\n",
    "            score = score + similarityScore\n",
    "    #   in case score = none  \n",
    "    if not score:\n",
    "        score = 0\n",
    "    return score/len(aspectsKeywords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Preprocessing :: Stopwords, pos taging, adjective removal\n",
    "'''\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessComment(sentance):\n",
    "#     removes stopwords \n",
    "    sentance = [word for word in word_tokenize(sentance) if word not in stopWords]\n",
    "#     removes punctuations \n",
    "    sentance = [word for word in sentance if word not in string.punctuation]\n",
    "#     pos taging\n",
    "    sentance = pos_tag(sentance)\n",
    "#     removes general adjective \n",
    "    return [word for (word, tag) in sentance if tag != 'JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25 25 50 50 50 50 75 75 75 75 75 75 100 100 125 125 125 125 125 125 150 150 150 150 150 150 150 175 175 175 175 175 175 175 175 200 200 200 200 200 225 225 225 225 225 225 225 250 250 250 250 250 250 250 275 275 275 275 275 275 275 300 300 300 300 300 300 300 325 325 325 325 350 350 350 350 350 375 375 375 375 375 375 400 400 400 400 400 400 400 425 425 425 425 425 425 425 450 450 450 450 475 475 475 475 475 475 475 475 475 500 500 500 500 500 525 525 525 525 525 525 525 550 550 550 550 550 550 550 550 575 575 575 575 575 575 575 600 600 600 600 600 625 625 625 625 625 625 650 650 650 650 650 650 675 675 675 675 700 700 700 700 725 725 725 725 725 750 750 750 750 750 750 750 750 775 775 775 775 775 775 775 800 800 800 800 800 800 800 825 825 825 825 825 825 825 825 825 825 850 850 850 850 850 850 850 850 875 875 875 875 875 900 900 900 900 900 925 925 925 925 925 925 950 950 \n",
      "PROCESS COMPLETE\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Read files and for each file it preprocess comment( use preprocessComment func ),\n",
    "    classify it (use classifier func) and finally write it in the correct aspect file \n",
    "'''\n",
    "for review in range(1, NO_OF_FILES+1):\n",
    "    with open(os.path.join(dataFilesPath, f'file{review}.txt'), 'r', encoding=\"utf-8\") as file:\n",
    "        # Read the file and lowercase the sentances at the same time and sentance tokenize it\n",
    "        comment = sent_tokenize(file.read().lower())\n",
    "        for sentance in comment:\n",
    "            processedSentance = preprocessComment(sentance)\n",
    "            aspectId = classifier(processedSentance)\n",
    "            try:\n",
    "                with open(os.path.join(preprocessFilesPath, f'{aspects[aspectId]}.txt'), 'a', encoding=\"utf-8\") as file:\n",
    "                    file.writelines(\" \".join(processedSentance)+\"\\n\")\n",
    "                with open(os.path.join(unprocessFilesPath, f'{aspects[aspectId]}.txt'), 'a', encoding=\"utf-8\") as file:\n",
    "                    file.writelines(sentance+\"\\n\")\n",
    "                if(review%25 == 0):\n",
    "                    print(review, end =\" \")        #Show no. of reviews processed\n",
    "            except:\n",
    "                print(\"Someting goes wrong while writing in file\")\n",
    "print(\"\\nPROCESS COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listWord = 'travel'#'Bazaar'\n",
    "# word = 'trip'\n",
    "# a = wordnet.synsets(listWord)[0]\n",
    "# b = wordnet.synsets(word)[0]\n",
    "# print('a',a,' b',b)\n",
    "# s = b.wup_similarity(a)\n",
    "# p = a.wup_similarity(b)\n",
    "# print(s or p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually Defined Aspects and their frequently occurring keywords\n",
    "\n",
    "# aspects = ['Value', 'Location', 'Service', 'Meal', 'Facility', 'Room', 'Quality', 'Staff', 'Surrounding', 'Others']\n",
    "\n",
    "# aspectsKeywords = [['Value', 'Price', 'Amount', 'Rate', 'Cheap', 'Worth', 'Low', 'Money', 'Economical', 'Reasonable', 'Fee', 'Expensive'],\n",
    "#  ['Location', 'Railway', 'View', 'Station', 'Airport', 'Distance', 'Far', 'Close', 'Convenient', 'Train', 'Metro', 'spot', 'Bazaar'],\n",
    "#  ['Service', 'Desk', 'Check-in', 'Checkout', 'Reliable', 'Fast', 'Convenient', 'Pick-up'],\n",
    "#  ['Meal', 'Drink', 'Breakfast', 'Spicy', 'Food', 'Tasty', 'Tea', 'Buffet', 'Bar', 'Restaurant', 'Dinner', 'Lunch', 'Brunch', 'Delicious'],\n",
    "#  ['Facility', 'Pool', 'Spa', 'Wifi', 'Gymnasium', 'Gym', 'Internet', 'Ample', 'Parking', 'Wireless', 'Broken'],\n",
    "#  ['Room', 'Bed', 'Dirty', 'Clean', 'Toilet', 'Bathroom', 'Shower', 'Dryer', 'Fridge', 'View'],\n",
    "#  ['Quality', 'Satisfactory', 'Ample', 'Hygienic', 'Proper', 'Ambience', 'Odour', 'Smell'],\n",
    "#  ['Staff', 'Good', 'Polite', 'Helpful', 'Friendly', 'Reliable', 'Quick', 'reception'],\n",
    "#  ['Surrounding', 'Landmark', 'Monument', 'Temple', 'Mosque', 'Church', 'Restaurant', 'Diner', 'Mall', 'Market', 'spot'],\n",
    "#  ['Paharganj', 'Delhi', 'Stay', 'Visit', 'Back', 'Driver', 'Booked', 'Agent', 'Trip']]\n",
    "# #  'India', 'Taxis', 'Drive', 'Foreigner', 'Landed', \n",
    "# # creating empty aspects files\n",
    "# for aspect in aspects:\n",
    "#     try:\n",
    "#         with open(os.path.join(preprocessFilesPath, f'{aspect}.txt'), 'w', encoding=\"utf-8\") as file:\n",
    "#             file.write('')\n",
    "#         with open(os.path.join(unprocessFilesPath, f'{aspect}.txt'), 'w', encoding=\"utf-8\") as file:\n",
    "#             file.write('')\n",
    "#     except:\n",
    "#         print(\"Could not write file. Try again\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
